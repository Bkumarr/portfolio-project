{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMdbwOEbeQ0s"
   },
   "source": [
    "# Milestone 5 - Applying Semantic Segmentation on Art images\n",
    "\n",
    "---  \n",
    "\n",
    "Image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects). Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.\n",
    "\n",
    "The first section of this notebook will help you to familiarise yourself with the use of pre-trained segmentation models. Globally, these models tend to perform well but the segmentation can be improved by using additional art images. The second section will show how these pre-trained models can be refined.\n",
    "\n",
    "> **Note 1:** During this milestone, only the `genre painting` images will be used. The scope reduction is intentional as we want you to focus on the understanding of the underlying concepts.\n",
    "\n",
    "> **Note 2:** The content of this notebook is very similar to the previous notebooks you had to work on. We have kept the same structure to facilitate the comprehension.\n",
    "\n",
    "  \n",
    "## Objective of this notebook\n",
    "* Applying Semantic Segmentation on images from Art Paintings dataset using pre-trained DeepLabV3 model to detect various object boundaries.\n",
    "* Train pre-trained DeepLabv3 model using transfer learning on given art images dataset.\n",
    "\n",
    "> **Note:** The content of this notebook follows the description provided in [Milestone-5](./Milestone-5.md)\n",
    "\n",
    "---\n",
    "## 1. Prerequisites\n",
    "\n",
    "### Importing packages & modules\n",
    "\n",
    "You might prefer to load the required modules/packages when required. Feel free to do so if it is your preference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1609249725563
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Common modules/packages\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import sys, time\n",
    "import warnings\n",
    "\n",
    "# PyTorch modules/packages\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Setting up the training mode based on CUDA capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1609249725766
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "device = 'cpu'  # sets the default value\n",
    "train_on_gpu = torch.cuda.is_available()  # returns True if CUDA enabled GPU is available\n",
    "\n",
    "if train_on_gpu == True :\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    print('\\n')\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Define some utilities functions  \n",
    "These functions will be used to load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1609249725837
    }
   },
   "outputs": [],
   "source": [
    "# Retrieves the list of files with a directory\n",
    "def getFilesInDirectory(pathToDir, extension = \"*.*\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return list(pathToDir.glob(extension))\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFoldersInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFolderNamesInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld.name for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])\n",
    "\n",
    "# Calculates the Intersection Over Union (IOU)\n",
    "def iou(pred, target, n_classes = 3):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    # Ignore IoU for background class (\"0\")\n",
    "    for cls in range(1, n_classes):  # This goes from 1:n_classes-1 -> class \"0\" is ignored\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()  # Cast to long to prevent overflows\n",
    "        union = pred_inds.long().sum().data.cpu().item() + target_inds.long().sum().data.cpu().item() - intersection\n",
    "        if union > 0:\n",
    "            ious.append(float(intersection) / float(max(union, 1)))\n",
    "\n",
    "    return np.array(ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 1 - PRE-TRAINED MODEL\n",
    "---\n",
    "## 1. Prepare the dataset\n",
    "\n",
    "For the sake of simplicity, we will limit our input to the `genre painting` images. The set of images can found in the `segmented\\genre` folder. It will be used as input to the models. In this section, the model will output the segmented images to the folder `segmented\\outputs_s1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1609249726304
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE DATA DIRECTORIES & LOCATION OF IMAGE-SETS ARCHIVE\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# sets the root folder for image sets\n",
    "pathToDataset = pathlib.Path.cwd().joinpath('..','dataset')\n",
    "pathToTest = pathToDataset.joinpath('test')\n",
    "\n",
    "# Check 'segmented' folder exists\n",
    "pathToSegmented = pathToDataset.joinpath('segmented')\n",
    "if not pathToSegmented.exists():\n",
    "    pathToSegmented.mkdir()\n",
    "\n",
    "pathToSegmentedImg = pathToSegmented.joinpath('genre', 'train')\n",
    "if not pathToSegmentedImg.exists():\n",
    "    pathToSegmentedImg.mkdir()\n",
    "\n",
    "pathToSegmentedOutput1 = pathToSegmented.joinpath('outputs_s1')\n",
    "if not pathToSegmentedOutput1.exists():\n",
    "    pathToSegmentedOutput1.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Segmentation\n",
    "\n",
    "`DeepLabV3 with Resnet-101 backbone` model will be used to deal with image semantic segmentation. This pre-trained model have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n",
    "\n",
    "The pre-trained model was trained on `21` classes and thus our output will have `21` channels ! We need to convert this 21 channeled outputs into a `2D` image (or a `1` channeled image), where each pixel of that image corresponds to a class.  \n",
    "\n",
    "### 2.1. Segmentation Mapping\n",
    "At some point, we will need to convert to a segmentation map an image where each pixel corresponds to a class label. Each class label will be converted into a `RGB` color. The purpose is to visualise easier the segmentation.\n",
    "\n",
    "---\n",
    "># TO DO: \n",
    ">* Let's define a function (called `decode_segmap`) that would accept an `2D` image and the colors for each categories handled by the model.\n",
    ">* The function should now create an `RGB` image from the `2D` image passed. To do so, the function creates empty `2D` matrices for all 3 channels. So, `r`, `g`, and `b` are arrays which will form the `RGB` channels for the final image. And each are of shape `[H x W]` (which is same as the shape of `image` passed in)\n",
    ">* The function will then loop over each class color we stored in `label_colors`, get the indexes in the image where that particular class label is present and  for each channel, it puts its corresponding color to those pixels where that class label is present.\n",
    ">* Finally the function stacks the 3 separate channels to form a `RGB` image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "gather": {
     "logged": 1609249726477
    },
    "id": "5GA_GNohUHnR"
   },
   "outputs": [],
   "source": [
    "def decode_segmap(image, label_colors):\n",
    "\n",
    "    len_categories = ...\n",
    "    \n",
    "    r = ...\n",
    "    g = ...\n",
    "    b = ...\n",
    "\n",
    "    for l in range(0, len_categories):\n",
    "        ...\n",
    "\n",
    "    rgb = ...\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhUMhvU4eQ2H"
   },
   "source": [
    "### 2.2. Image Pre-processing\n",
    "\n",
    "The images should be preprocessed to facilitate the segmentation process. Let's define a function that will transform the images according to the model input expectations. \n",
    "\n",
    "* The function will first `label_colors` stores the colors for each of the classes, according to the index\n",
    "    * The first class which is `background` is stored at the `0th` position (index)\n",
    "    * The second class which is `aeroplane` is stored at the `1st` position (index)\n",
    "    * ...\n",
    "\n",
    "* Open the file and convert it to RGB  \n",
    "\n",
    "* The function should unsqueeze the image so that it becomes `[1 x C x H x W]` from `[C x H x W]`  \n",
    "\n",
    "* The `2D` image, (of shape `[H x W]`) will have each pixel corresponding to a class label. So, each `(x, y)` will correspond to a number between `0 - 20` representing a class (`[1 x 21 x H x W]`). The function should take a max index for each pixel position.\n",
    "\n",
    "> **Note:** The pre-trained model that will be used has been trained on `21 categories` (20 categories + background (black)).\n",
    "> The classes that the pre-trained model outputs are the following, in order:  \n",
    ">  ['__background__' , 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    " 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    " 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "> **Note:** The function will require a transformation which:  \n",
    ">    * resize the image to `(256 x 256)`\n",
    ">    * center and crop it to `(224 x 224)`\n",
    ">    * convert it to a Tensor (all the values in the image becomes between `[0, 1]` from `[0, 255]`)\n",
    ">    * normalize it with the Imagenet specific values `mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]`  \n",
    "\n",
    "---\n",
    "># TO DO: \n",
    ">* In the Segmentation function:\n",
    ">     * Apply the segmentation to the image\n",
    ">     * Execute the model against the tensor (output of the transformation) and collect the `out` tensor\n",
    ">     * Collect the indices of the maximum value of all elements in the  `out` tensor\n",
    ">     * Call the `decode_segmap` function with the tensor ( using numpy() ) and the label_colors  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "gather": {
     "logged": 1609249726673
    },
    "id": "_yTvVvX7eQ32"
   },
   "outputs": [],
   "source": [
    "# Define the data-augmentation transforms including normalisations\n",
    "segment_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "label_colors = np.array([\n",
    "              (0, 0, 0),  # 0=background\n",
    "              (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),         # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "              (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),       # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "              (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "              (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)           # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "           ])\n",
    "\n",
    "\n",
    "\n",
    "def segment(seg_model, fileName, outputPath, showImage=True):\n",
    "\n",
    "    local_img = Image.open(fileName).convert('RGB')\n",
    "    n_img = np.asarray(local_img)\n",
    "    \n",
    "    if len(n_img.shape) != 3:\n",
    "        print('ERROR! ', file, ' is grayscale and not in RGB format. Cannot implement segmentation. Ignoring it!')\n",
    "    else:\n",
    "        print('Implementing segmentation on ', fileName)\n",
    "\n",
    "        inp = ...\n",
    "        out = ...\n",
    "        image = ...\n",
    "        rgb = ...\n",
    "        \n",
    "        if (showImage == True):\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(local_img)\n",
    "            plt.subplot(122)\n",
    "            plt.imshow(rgb)\n",
    "            plt.axis('off');\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-r1Pac90eQ1R"
   },
   "source": [
    "### 2.3. Model-based Sematic Segmentation\n",
    "\n",
    "1. Load in a pre-trained model & display its structure \n",
    "2. Process each file for each art category under `Test` folder, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "colab_type": "code",
    "gather": {
     "logged": 1609249409337
    },
    "id": "jebN_lm9eQ1W",
    "outputId": "d016f186-00f8-4818-e5d6-d42b654bdd64"
   },
   "outputs": [],
   "source": [
    "# Set the model\n",
    "deep_lab = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n",
    "\n",
    "# Print out the model\n",
    "#print( deep_lab )\n",
    "\n",
    "# Collect the images\n",
    "files = getFilesInDirectory(pathToSegmentedImg, '*.jpg')\n",
    "for file in files:\n",
    "    # Apply semantic segmentation\n",
    "    segment(deep_lab, file, pathToSegmentedOutput1, showImage=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 2 - TRANSFER LEARNING BASED ON PRE-TRAINED MODEL\n",
    "---\n",
    "The method used in Section 1 gives very good result considering that the pre-trained model has been trained through photographs. Let's see now if we can improve the performance by applying some transfer learning.\n",
    "\n",
    "## 1. Data Preparation\n",
    "\n",
    "For the sake of simplicity, we will limit our input to the `genre painting` images. The set of images can found in the `segmented\\genre` folder which should have been created during Milestone 1. It will be used as input to the models. In this section, the model will require some ground truth masks - found under `segmented\\masks`.\n",
    "\n",
    "### 1.1. Customised Dataset\n",
    "\n",
    "We will create a class representing a customised dataset (called `SegmentationDataset`) for image segmentation task in order to bundle data and functionality together. The class should extend the `Dataset` class (from `torch.utils.data.dataset`). It will contain the transformation definition for training and validation.\n",
    "\n",
    "Three methods will be required:\n",
    "* **__init__()**: should receive the path to the images and the ground truth masks and the mode (Training, Validation)\n",
    "* **__len__()**: should return the number of the images\n",
    "* **__getitem__**: should convert a combination of images and masks, transform the combination and return the image and the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_path, mask_path, mode):\n",
    "        super(SegmentationDataset, self).__init__()\n",
    "\n",
    "        # Collect files\n",
    "        self.img_files = glob.glob(os.path.join(img_path,'*.jpg'))\n",
    "        self.msk_files = glob.glob(os.path.join(mask_path,'*.png'))\n",
    "\n",
    "        # Data augmentation and normalization for training\n",
    "        # Just normalization for validation (='V')\n",
    "        if \"V\" == mode :\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.CenterCrop((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406, 0], [0.229, 0.224, 0.225, 1])\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.RandomCrop((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406, 0], [0.229, 0.224, 0.225, 1])\n",
    "                ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            msk_path = self.msk_files[index]\n",
    "\n",
    "            image = Image.open(img_path)\n",
    "            mask = Image.open(msk_path)\n",
    "\n",
    "            # Convert to arrays\n",
    "            image_np = np.asarray(image)\n",
    "            mask_np = np.asarray(mask)\n",
    "            \n",
    "            # Convert to tuple (256, 256, 4)\n",
    "            new_shape = (image_np.shape[0], image_np.shape[1], image_np.shape[2] + 1)\n",
    "            \n",
    "            # Convert to ndarray (256, 256, 4)\n",
    "            combined_np = np.zeros(new_shape, image_np.dtype)\n",
    "            \n",
    "            # Concatenate image and mask so transformation is applied on both\n",
    "            combined_np[:, :, 0:3] = image_np\n",
    "            combined_np[:, :, 3] = mask_np\n",
    "\n",
    "            # Convert to PIL\n",
    "            combined = Image.fromarray(combined_np)\n",
    "\n",
    "            # Apply transformation and get a Tensor [4, 224, 224]\n",
    "            combined = self.transforms(combined)\n",
    "\n",
    "            # Extract image Tensor ([3, 224, 224]) and mask Tensor ([1, 224, 224])\n",
    "            image = combined[0:3, :, :]\n",
    "            mask = combined[3, :, :].unsqueeze(0)\n",
    "            \n",
    "            # Normalize back from [0, 1] to [0, 255]\n",
    "            mask = mask * 255\n",
    "\n",
    "            #  Convert to int64 and remove second dimension - Tensor ([224, 224])\n",
    "            mask = mask.long().squeeze()\n",
    "\n",
    "            return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Path definition\n",
    "Define the path to the Ground Truth masks. The path for images has already been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks folder\n",
    "pathToSegmentedMask = pathToSegmented.joinpath('masks', 'train')\n",
    "if not pathToSegmentedMask.exists():\n",
    "    pathToSegmentedMask.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Transformation & Data loaders\n",
    "Some data loader's parameters will be set: the size of the batch to 4 and the number of workers to 0.\n",
    "\n",
    "We will first instantiate the `SegmentationDataset` class with the appropriate parameters and, then, create a dataloader using the dataset instance.\n",
    "\n",
    "**Note**: Don't forget to set to `True` the **drop_last** parameter of the Dataloader constructor.\n",
    "\n",
    "---\n",
    "># TO DO: \n",
    ">* Instantiate the `SegmentationDataset` class using the paths defined above and the mode\n",
    ">* Create a dataloader using the following arguments:\n",
    ">     * the dataset\n",
    ">     * the size of the batch\n",
    ">     * activate the drop_last mode to avoid processing batch with only 1 element\n",
    ">     * activate the shuffle mode\n",
    ">     * the number of workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 0\n",
    "\n",
    "# 'T'rain & 'V'alidation transformations & data loaders\n",
    "tv_datasets = ...\n",
    "tv_dataloaders = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelling\n",
    "\n",
    "Let's set the pretrained model based on which the transfer learning will be performed. Then we will set the different parameters of the features. The model will then be customized to fit specific requirements related to Semantic Segmentation. This is very similar to what we have seen in the previous milestones.\n",
    "\n",
    "### 2.1. Load and configure the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the number of output channels (= number of classes)\n",
    "num_classes = 7\n",
    "\n",
    "# Model definition\n",
    "seg_model = models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n",
    "\n",
    "# The auxiliary classifier is removed, and the pretrained weights are frozen\n",
    "seg_model.aux_classifier = None\n",
    "for param in seg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# The pretrained classifier is replaced by a new one with a custom number of classes.\n",
    "# Since it comes after the freeze, its weights won't be frozen. They are the ones that we will fine-tune.\n",
    "seg_model.classifier = DeepLabHead(2048, num_classes)\n",
    "\n",
    "# Model serialisation\n",
    "model_filename = 'custom_segmented.pt'\n",
    "pathToModel = pathlib.Path.cwd().joinpath('..', 'models', model_filename)\n",
    "print('File name for saved model: ', pathToModel)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss() # combines nn.LogSoftmax() and nn.NLLLoss(), well suited for multiclass classification problems\n",
    "\n",
    "# Optimizer definition\n",
    "optimizer = torch.optim.SGD(seg_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Use cpu/gpu based on availability\n",
    "seg_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Setup variables used during Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "counter = 0\n",
    "\n",
    "# Initialise epoch performances\n",
    "epoch_loss = 0.0\n",
    "epoch_acc, epoch_bestacc = 0.0, 0.0\n",
    "\n",
    "# Initialise training performances\n",
    "train_iou_means, train_losses = [], []\n",
    "train_loss, train_iou_mean = 0.0, 0.0\n",
    "\n",
    "# Initialise validation performances\n",
    "val_acc_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Training & Validation\n",
    "\n",
    "The structure of the Training & Validation phases has been simplified compared to the previous milestones.\n",
    "\n",
    "**Training**  \n",
    "The training will be performed by going through every epoch.  \n",
    "\n",
    "* Set the `training` mode\n",
    "* Collect a set of images and for each image:\n",
    "    * activate the automated calculation of gradients\n",
    "    * execute the model\n",
    "    * calculate the loss using the criterion\n",
    "    * apply a backward pass and single step optimization\n",
    "    * collect the Intersection Over Union metric using the function defined at the beginning of this notebook\n",
    "\n",
    "**Validate**  \n",
    "The validation will be performed by going through every epoch - same as the training.\n",
    "\n",
    "* Set the `evaluation` mode\n",
    "* Place the model into validation model\n",
    "    * execute the model\n",
    "    * calculate the loss using the criterion\n",
    "    * collect the Intersection Over Union metric using the function defined at the beginning of this notebook\n",
    "\n",
    "Lastly, for each epoch, print the performance variables and save the model if the validation loss has decreased.\n",
    "\n",
    "---\n",
    "># TO DO: \n",
    ">* Set the mode for the model\n",
    ">* Collect the maximum (prediction) values from the `outputs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()  # Start-time for training\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # Start-time for epoch\n",
    "    c = time.time()\n",
    "\n",
    "    for phase in ['T', 'V']:\n",
    "        if phase == 'T':\n",
    "            # Set to training mode\n",
    "            ...\n",
    "        else:\n",
    "            # Set to evaluation mode\n",
    "            ...\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_iou_means = []\n",
    "        \n",
    "        # Iterate over data - Getting one batch of training images and their corresponding true labels\n",
    "        for inputs, masks in tv_dataloaders[phase]:\n",
    "\n",
    "            images = inputs.to(device) # Array of tensors - size: [3, 224, 224]\n",
    "            masks = masks.to(device) #  Array of tensors - size: [224, 224]\n",
    " \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Turning on calculation of gradients (not required for validation)\n",
    "            with torch.set_grad_enabled(phase == 'T'):\n",
    "                \n",
    "                outputs = seg_model(images)['out'] # returns an Array of tensors - size: [3, 224, 224]\n",
    "\n",
    "                # Calculate the train loss\n",
    "                train_criterion = criterion(outputs, masks)    # (prediction, true-label)\n",
    "                train_loss += train_criterion.item() * inputs.size(0)\n",
    "\n",
    "                # Returns the maximum values (tuple: values, indices) of each row of the `outputs` in the dimension `1`\n",
    "                ...\n",
    "\n",
    "            if (phase == 'T'):\n",
    "                \n",
    "                # backward pass: compute gradient of the loss based on model parameters\n",
    "                train_criterion.backward()\n",
    "\n",
    "                # perform a single optimization step\n",
    "                optimizer.step()\n",
    "\n",
    "            # Collect the Intersection Over Union (IOU)\n",
    "            train_iou_mean = iou(preds, masks, num_classes).mean()\n",
    "            train_iou_means.append(train_iou_mean)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            # Increment counter\n",
    "            counter = counter + 1\n",
    "\n",
    "        # Displays statistics\n",
    "        epoch_loss = train_loss / len(tv_dataloaders[phase].dataset)\n",
    "        if (train_iou_means is not None):\n",
    "            epoch_acc = np.array(train_iou_means).mean()\n",
    "        else:\n",
    "            epoch_acc = 0.\n",
    "\n",
    "        d = time.time() # end-time for epoch\n",
    "\n",
    "        print(f\"Epoch: {epoch} | \"\n",
    "              f\"Time: {int((d-c)/60)} min {int(d-c)%60} sec | \"\n",
    "              f\"Phase: {phase} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\"\n",
    "             )\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if ((phase == 'V') and (epoch_acc > epoch_bestacc)):\n",
    "            print('Epoch accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'.format(epoch_bestacc , epoch_acc))\n",
    "\n",
    "            # saving model\n",
    "            torch.save(seg_model.state_dict(), pathToModel)\n",
    "\n",
    "            # update minimum validation loss\n",
    "            epoch_bestacc = epoch_acc\n",
    "\n",
    "        if (phase == 'V'):\n",
    "            val_acc_history.append(epoch_acc)\n",
    "\n",
    "################\n",
    "# END OF EPOCH #\n",
    "################\n",
    "b = time.time()\n",
    "print('\\n\\n\\tTotal training time: ' , int((b-a)/(60*60)), \"hour(s) \" , int(((b-a)%(60*60))/60),\"minute(s) \", int(((b-a)%(60*60))%60) , \"second(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the model  \n",
    "The model saved has the lowest validation loss. It is that model that should be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.load_state_dict(torch.load(pathToModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Testing\n",
    "\n",
    "**Preparation**  \n",
    "\n",
    "* The following variables will be defined:\n",
    "    * The path where the test images are located\n",
    "    * The transformation that will be applied to the test images\n",
    "    * the list of labels potentially present on the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test & Valid image folder\n",
    "pathToSegmentedTest = pathToSegmented.joinpath('genre', 'test')\n",
    "if not pathToSegmentedTest.exists():\n",
    "    pathToSegmentedTest.mkdir()\n",
    "\n",
    "# Test transform & data loader\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define the list of labels to be identified\n",
    "label_colors = np.array([\n",
    "            (0, 0, 0),  # 0= background\n",
    "            (128, 0, 0), # 1= chair\n",
    "            (0, 128, 0), # 2= door_window\n",
    "            (128, 128, 0), # 3= person\n",
    "            (0, 0, 128),  # 4= table\n",
    "            (128, 0, 128), # 5= animal\n",
    "            (0, 128, 128) # 6= bird\n",
    "])\n",
    "\n",
    "# Set the model to evaluate mode\n",
    "seg_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**  \n",
    "\n",
    "For each images present in the test folder:\n",
    "* Convert the image to RGB\n",
    "* Apply the transformation to the image and add a dimension to fit into the model input\n",
    "* Execute the model\n",
    "* Identify and colorise the segmented area based on `label_colors`\n",
    "* Display the test image and the segmented mask produced by the model\n",
    "\n",
    "---\n",
    "># TO DO: \n",
    ">* Apply the transformation to the image and add a dimension to fit into the model input\n",
    ">* After reducing the number of dimension, returns the indices of the maximum values of a tensor across a dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = getFilesInDirectory(pathToSegmentedTest, '*.jpg')\n",
    "\n",
    "for idx in range(0, len(images)-1):\n",
    "    \n",
    "    image_orig = Image.open(images[idx]).convert(\"RGB\")\n",
    "    n_img = np.asarray(image_orig)\n",
    "\n",
    "    if len(n_img.shape) != 3:\n",
    "        print('ERROR! ', file, ' is grayscale and not in RGB format. Cannot implement segmentation. Ignoring it!')\n",
    "    else:\n",
    "        print('Implementing segmentation on ', images[idx])\n",
    "\n",
    "        # Apply the transformation to the image and add a dimension to fit into the model input\n",
    "        inp = test_transforms(image_orig).unsqueeze(0).to(device) # inp= Size([1, 3, 256, 256])\n",
    "\n",
    "        # Execute the model and return the output (`out`)\n",
    "        out = seg_model.to(device)(inp)['out'] # out= Size([1, 7, 256, 256])\n",
    "        \n",
    "        # Returns the indices of the maximum values of a tensor across a dimension.\n",
    "        image = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "        \n",
    "        # Colorise the image with the predefined labels using the `decode_segmap`\n",
    "        rgb = decode_segmap(image, label_colors)\n",
    "\n",
    "        # Plotting\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(image_orig)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off');\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "intro-seg.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
