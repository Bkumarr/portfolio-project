{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 - Implementing Transfer Learning on Art images dataset using  pre-trained models  \n",
    "\n",
    "---  \n",
    "\n",
    "# Objective of this notebook\n",
    "* Train pre-trained alexnet, vgg19 and resnet50 models using transfer learning on given art images dataset to compare and visualize their performance on different classes.\n",
    "\n",
    "> **Note:** The content of this notebook follows the description provided in [Milestone-2](./Milestone-2.md)\n",
    "\n",
    "---\n",
    "## 1. Prerequisites\n",
    "\n",
    "### Importing packages & modules\n",
    "\n",
    "You might prefer to load the required modules/packages when required. Feel free to do so if it is your preference\n",
    "\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common modules/packages\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import sys, shutil, time\n",
    "import warnings\n",
    "\n",
    "# PyTorch modules/packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training mode based on CUDA capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'  # sets the default value\n",
    "train_on_gpu = torch.cuda.is_available()  # returns True if CUDA enabled GPU is available\n",
    "\n",
    "if train_on_gpu == True :\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    print('\\n')\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "    device = 'cuda'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the list of files with a directory\n",
    "def getFilesInDirectory(pathToDir, extension = \"*.*\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return list(pathToDir.glob(extension))\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFoldersInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFolderNamesInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld.name for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "The folder structure created through Milestone 1 should looks like this:\n",
    "```\n",
    "dataset/train/artCategory_1/file_01.jpg\n",
    "dataset/train/artCategory_1/file_03.jpg\n",
    "dataset/train/artCategory_1/file_06.jpg\n",
    "...\n",
    "dataset/test/artCategory_1/file_02.jpg\n",
    "...\n",
    "dataset/valid/artCategory_1/file_04.jpg\n",
    "```\n",
    "\n",
    "The root folder for training is `dataset/train` and the classes are the names of art types.\n",
    "Likewise, `dataset/valid` and `dataset/test` for validation and testing respectively.\n",
    "\n",
    "> **Note:**\n",
    ">   - If you have downloaded images through Milestone 1, the expected folder structure above should be already present\n",
    ">   - If you have used the set of images (image-sets.zip) provided, please note these images have the following format: 432*288\n",
    "\n",
    "### Sets the folders\n",
    "* Set the location for `train`, `test` and `valid` folders\n",
    "* Count the number of Art category and list them using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE DATA DIRECTORIES & LOCATION OF IMAGE-SETS ARCHIVE\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# sets the root folder for image sets\n",
    "pathToDataset = pathlib.Path.cwd().joinpath('..', 'dataset')\n",
    "pathToTrain = pathToDataset.joinpath('train')\n",
    "pathToTest = pathToDataset.joinpath('test')\n",
    "pathToValid = pathToDataset.joinpath('valid')\n",
    "\n",
    "# list the folders required under 'dataset' folder (using a list to reduce the lines of code)\n",
    "artCategories = getFolderNamesInDirectory(pathToTrain, \".\")  #collects the list of folders\n",
    "print(\"Total no. of categories = \", len(artCategories))  #displays the number of classes (= Art categories)\n",
    "print(\"Categories: \", artCategories)  #displays the list of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Let's assume that our model expects a `224`-dim square image as input. Resizing will therefore be required for each art image to fit this model. These transformations applies on `train`, `test` and `valid`. Use PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class, which makes it very easy to load data from a directory. \n",
    "\n",
    "***A. Data-Augmentation***  \n",
    "Using data augmentation on training images:\n",
    "\n",
    "1. Randomly rotating the training by 30 degrees.\n",
    "2. Randomly cropping and resizing it to 224-dim square image.  \n",
    "    !!! Please use only one function that crops the given image to random size and aspect ratio.\n",
    "3. Randomly flipping it horizontally.\n",
    "\n",
    "> **Note:** \n",
    "Data augmentation isn't applied on validation and testing set. These images are resized to 256 pixels and then cropped from center to make it 224-dim square images.  \n",
    "\n",
    "***B. Normalization:***  \n",
    "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. Same normalization was used by these pre-trained models for training.\n",
    "\n",
    "---\n",
    "># TO DO (1):\n",
    ">## Implement the transformations in the next code paragraph based on the explanation above \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data-augmentation transforms including normalisations\n",
    "train_transforms = transforms.Compose([...,\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ...])\n",
    "                 \n",
    "test_transforms  = transforms.Compose([...,\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ...])\n",
    "                 \n",
    "valid_transforms = transforms.Compose([...,\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ...])\n",
    "\n",
    "# load and apply above transforms on dataset using ImageFolder\n",
    "train_dataset = ...\n",
    "test_dataset  = ...\n",
    "valid_dataset = ...\n",
    "\n",
    "# Print out data stats\n",
    "print('Training  images: ', len(train_dataset))\n",
    "print('Testing   images: ', len(test_dataset))\n",
    "print('Validation images:', len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "A [data loader](https://pytorch.org/docs/stable/data.html) is an iterable over a dataset. The parameters are:\n",
    "* `batch`:  number of samples per batch to be loaded\n",
    "* `shuffle`: set to True, data are reshuffled at every epoch.\n",
    "* `num_workers`: nbr of subprocesses to use for data loading\n",
    "\n",
    "---\n",
    "># TO DO (2):\n",
    ">## Implement the dataloaders in the next code paragraph based on the explanation above.\n",
    ">* Think on which dataset shuffle should only be applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloader parameters\n",
    "batch_size = 16  # You might want to increase the size to 32. This might raise an exception \n",
    "\n",
    "num_workers=0\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataloader = ...\n",
    "test_dataloader  = ...\n",
    "valid_dataloader = ...\n",
    "\n",
    "# Print the batches stats\n",
    "print('Number of  training  batches:', len(train_dataloader))\n",
    "print('Number of  testing   batches:', len(test_dataloader))\n",
    "print('Number of validation batches:', len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional - Data visualization\n",
    "> **Note:** the below code is not part of the model training/testing.\n",
    "\n",
    "---\n",
    "># TO DO (3 - Optional but recommended):\n",
    ">## Based on what you have learned above:\n",
    "> 1. Implement the transformation\n",
    "> 2. Apply the transformation on the dataset using ImageFolder\n",
    "> 3. Prepare the dataloader\n",
    "> * Think on which dataset shuffle should only be applied\n",
    "> 4. Get a numpy-version of the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore normalization and turn shuffle ON to visualize different art categories together\n",
    "visual_transforms  = transforms.Compose([...])\n",
    "\n",
    "# Load and apply above transform on dataset using ImageFolder\n",
    "# Use test directory images can be used for visualization\n",
    "visual_dataset  = ...\n",
    "\n",
    "# Prepare data loaders\n",
    "visualization_dataloader = ...\n",
    "\n",
    "# Obtain one batch of testing images\n",
    "dataiter = iter(visualization_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Convert images to numpy for display\n",
    "images = ...\n",
    "\n",
    "# Plot the images in the batch along with the corresponding labels\n",
    "plotCols = 4\n",
    "plotRows = math.ceil(batch_size/plotCols) # SqRoot could be used as well: math.ceil(math.sqrt(batch_size))\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(plotRows, plotCols, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(artCategories[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Modelling\n",
    "\n",
    "### 3.1. Definition\n",
    "\n",
    "Several pre-trained models are available and can be used directly. We have selected `resnet50`, `alextnet` and `vgg19`. In this section, we will set the pretrained model to be used, download and load it. Then we will set the different parameters of the features. The model will then be customized to fit specific requirements related to Art classification.\n",
    "\n",
    "1. Load in a pre-trained model & display its structure  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model to use\n",
    "model_name = 'alexnet' # resnet50, vgg19\n",
    "\n",
    "# Load the pretrained models from pytorch\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', model_name, pretrained=True)\n",
    "\n",
    "# Print out the model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. \"Freeze\" all the parameters, so the network acts as a fixed feature extractor \n",
    "\n",
    "> **Note:** Freezing means that the parameters of features part in the pre-trained model will *not* change during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze training for all \"features\" layers\n",
    "if model_name == 'resnet50': #ResNet50 model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "else:\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Replace the last layer with a custom linear classifier layer with desirable output nodes\n",
    "\n",
    "After having the pre-trained feature extractor, modifying final-fully-connected classifier layer\n",
    "\n",
    "> This layer should produce an appropriate number of outputs for this painting classification task.\n",
    "\n",
    "You can access any layer in a pre-trained network by name and (sometimes) number.\n",
    "* For Alexnet/Vgg19: model`.classifier[6]` is the sixth layer in a group of layers named \"classifier\".\n",
    "* For ResNet: `resnet50.fc` is the only fully connected Linear layer.\n",
    "\n",
    "---\n",
    "># TO DO (4):\n",
    "># Based on the explanation above \n",
    "> 1. Print in/out features\n",
    "> 2. Set as the last layer of the model a linear layer based on:\n",
    ">   2.1. the pre-trained model features as input\n",
    ">   2.2. the number of art categories as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "n_inputs = None\n",
    "last_layer = None\n",
    "\n",
    "if model_name == 'resnet50': #ResNet50 model\n",
    "    ...\n",
    "else: #AlexNet, Vgg19\n",
    "    ...\n",
    "\n",
    "# if GPU is available, move the model to GPU\n",
    "if train_on_gpu:\n",
    "    model=model.to(device)\n",
    "\n",
    "# Print the new model architecture\n",
    "print(model)\n",
    "\n",
    "# Check to see that your last layer produces the expected number of outputs\n",
    "if model_name == 'resnet50':\n",
    "    print(model.fc.out_features)\n",
    "else:\n",
    "    print(model.classifier[6].out_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Set the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'trained_' + model_name + '.pt'\n",
    "pathToModel = pathlib.Path.cwd().joinpath('..', 'models', model_filename)\n",
    "print('File name for saved model: ', pathToModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "Below we'll use cross-entropy loss and stochastic gradient descent\\. Note that the optimizer accepts as input _only_ the trainable parameters.\n",
    "\n",
    "> **Note:** AlexNet is made up of 2 parts->  Features (Convolutional and max-pooling layers) and Classifier (Fully-connected layers). In transfer-learning, only the Classifier part is trained.\n",
    "\n",
    "---\n",
    "># TO DO (5):\n",
    "># Based on the explanation above \n",
    "> 1. Sets the criterion using categorical cross-entropy combining nn.LogSoftmax() and nn.NLLLoss() in one single class. \n",
    "> 2. Sets the appropriate optimizer based on the type of pre-trained models used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = ...\n",
    "\n",
    "# specifying optimizer and learning rate\n",
    "if model_name == 'resnet50':\n",
    "    optimizer = optim.Adam(..., lr=0.001)\n",
    "else:\n",
    "    optimizer = optim.Adam(..., lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Model Training\n",
    "\n",
    "1. Training Preparation  \n",
    "\n",
    "    The following variables will be defined:\n",
    "    * The number of epochs required for the training and validation of the model\n",
    "    * The minimal value for validation loss\n",
    "    * Some performance variables such as:\n",
    "        * the losses for training and validation\n",
    "        * the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some images in dataset were truncated (maybe corrupted)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Set number of epochs to train the model\n",
    "n_epochs = 40\n",
    "\n",
    "# Initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "# Initialise performances\n",
    "train_losses, valid_losses, accuracies=[],[],[]\n",
    "training_loss = 0.0\n",
    "validation_loss = 0.0\n",
    "accuracy = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train & Validation the model\n",
    "\n",
    "    **Training**  \n",
    "    The training will be performed by going through every epoch.  \n",
    "\n",
    "    * Collect a set of labelled images and for each image:\n",
    "        * execute the `forward` method\n",
    "        * calculate the loss using the criterion and the gradient of the loss\n",
    "        * apply a single step optimization\n",
    "        * refresh the training loss\n",
    "\n",
    "    **Validate**  \n",
    "    The validation will be performed by going through every epoch - same as the training.\n",
    "\n",
    "    * Place the model into validation model\n",
    "        * execute the `forward` method\n",
    "        * calculate the validation loss and the accuracy\n",
    "        * compare your predictions against the true labels\n",
    "        * incrementing values of 'accuracy' with equals\n",
    "    * Save the model if validation loss has decreased\n",
    "\n",
    "    Lastly, for each epoch, print the performance variables.\n",
    "\n",
    "---\n",
    "># TO DO (6):\n",
    "># Based on the explanation above \n",
    "> 1. Enumerate through a dataloader - think which data_loader should be used here\n",
    "> 2. Make sure the previous gradients of the optimized variables are cleared\n",
    "> 3. Calculate the batch loss based on the outputs & labels (in train & validation)\n",
    "> 4. Calculate the accuracy. The objective here is to return a new tensor based on output values. The tensor will turn outputs into probabilities using exponential function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()  #Start-time for training\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    c = time.time()  #Start-time for epoch\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    \n",
    "    # Getting one batch of images and their corresponding true labels\n",
    "    for batch_i, (images, labels) in enumerate(...):\n",
    "\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "        # clear the previous/buffer gradients of all optimized variables\n",
    "        ...\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model.forward(images)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = ...\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update training loss \n",
    "        training_loss += loss.item()   \n",
    "        \n",
    "    ##################\n",
    "    # VALIDATE MODEL #\n",
    "    ##################      \n",
    "    \n",
    "    model.eval() #model is put to evalution mode i.e. dropout is switched off\n",
    "\n",
    "    with torch.no_grad():  #Turning off calculation of gradients (not required for validaiton)  {saves time}\n",
    "        for images, labels in valid_dataloader:   #Getting one batch of validation images\n",
    "            \n",
    "            if train_on_gpu:   #moving data to GPU if available\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            outputs = model.forward(images)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            batch_loss = ...\n",
    "            validation_loss += batch_loss.item()\n",
    "            \n",
    "            # Calculating accuracy\n",
    "            ps = ...\n",
    "\n",
    "            #getting top one probablilty and its corresponding class for batch of images\n",
    "            top_p, top_class = ps.topk(1, dim=1) \n",
    "\n",
    "            #Comparing our predictions to true labels\n",
    "            equals = top_class == labels.view(*top_class.shape)   #equals is a list of values\n",
    "            #incrementing values of 'accuracy' with equals\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()   #taking average of equals will give number of true-predictions\n",
    "                                                #equals if of ByteTensor (boolean), changing it to FloatTensor for taking mean...\n",
    "    \n",
    "    \n",
    "    train_losses.append(training_loss/len(train_dataloader))    \n",
    "    valid_losses.append(validation_loss/len(valid_dataloader))\n",
    "    accuracies.append(((accuracy/len(valid_dataloader))*100.0))\n",
    "    d = time.time() #end-time for epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch} \"\n",
    "          f\"Time: {int((d-c)/60)} min {int(d-c)%60} sec \"\n",
    "          f\"Train loss: {training_loss/len(train_dataloader):.3f}.. \"\n",
    "          f\"Validation loss: {validation_loss/len(valid_dataloader):.3f}.. \"\n",
    "          f\"Validation accuracy: {((accuracy/len(valid_dataloader))*100.0):.3f}% \"\n",
    "          )\n",
    "    \n",
    "    training_loss = 0.0\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if ( validation_loss/len(valid_dataloader) <= valid_loss_min):\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min , validation_loss/len(valid_dataloader)))\n",
    "        torch.save(model.state_dict(), pathToModel) #Saving model \n",
    "        valid_loss_min = validation_loss/len(valid_dataloader)   #Minimum validation loss updated\n",
    "    \n",
    "    \n",
    "    #After validation, model is put to training mode i.e. dropout is again switched on\n",
    "    model.train()\n",
    "          \n",
    "    ################\n",
    "    # END OF EPOCH #\n",
    "    ################\n",
    "            \n",
    "b = time.time()  #end-time for training\n",
    "print('\\n\\n\\tTotal training time: ' , int((b-a)/(60*60)), \"hour(s) \" , int(((b-a)%(60*60))/60),\"minute(s) \", int(((b-a)%(60*60))%60) , \"second(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the model  \n",
    "    The model saved has the lowest validation loss. It is that model that should be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(pathToModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Testing\n",
    "\n",
    "1. Testing Preparation  \n",
    "\n",
    "    The following variables will be defined:\n",
    "    * The number of epochs required for the training and validation of the model\n",
    "    * The minimal value for validation loss\n",
    "    * Some performance variables such as:\n",
    "        * the losses for training and validation\n",
    "        * the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "counter = 0\n",
    "\n",
    "class_correct = list(0. for i in range(len(artCategories)))\n",
    "class_total = list(0. for i in range(len(artCategories)))\n",
    "classes_accuracies=[]\n",
    "\n",
    "# evaluation mode (switching off dropout)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Testing the model  \n",
    "\n",
    "    * Collect a set of labelled images and for each image:\n",
    "        * send each image to the model and collect the output\n",
    "        * based on the output and the label, evaluate the loss\n",
    "        * convert the output probabilities to a predicted class\n",
    "        * for each art category, calculate the test accuracy\n",
    "\n",
    "---\n",
    "># TO DO (7):\n",
    "># Based on the explanation above \n",
    "> 1. Iterate through a dataloader - think which data_loader should be used here\n",
    "> 2. Calculate the batch loss based on the outputs & labels (in train & validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# iterate over test data - get one batch of data from testloader\n",
    "for images, labels in ...:\n",
    "    \n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(images)\n",
    "    \n",
    "    # calculate the batch loss\n",
    "    loss = ...\n",
    "    \n",
    "    # update  test loss \n",
    "    test_loss += loss.item()*images.size(0)\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    ps, pred = torch.max(output, 1)    \n",
    "    \n",
    "    # compare model's predictions to true labels\n",
    "    for i in range(len(images)):\n",
    "        y_true.append( artCategories[labels[i]] )\n",
    "        y_pred.append( artCategories[pred[i]] )\n",
    "\n",
    "        class_total[labels[i]] += 1\n",
    "        if pred[i] == labels[i]:\n",
    "            class_correct[pred[i]] += 1\n",
    "    counter += 1\n",
    "    \n",
    "\n",
    "#calculate avg test loss\n",
    "test_loss = test_loss/len(test_dataloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(artCategories)):\n",
    "    classes_accuracies.append(100 * class_correct[i] / class_total[i])\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            artCategories[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    \n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (artCategories[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % \n",
    "                          (100. * np.sum(class_correct) / np.sum(class_total),np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "array = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "df_cm = pd.DataFrame(array, index = [i for i in artCategories],\n",
    "                  columns = [i for i in artCategories])\n",
    "plt.figure(figsize = (10,7))\n",
    "mx = sn.heatmap(df_cm/np.sum(df_cm), annot=True, fmt='.2%', cmap='Blues')\n",
    "del df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">\n",
    "\n",
    "# IMPORTANT NOTE\n",
    "\n",
    "You might find the accuracy of the models you have generated disappointing. This is due to the fact that these pre-trained models have been trained on photographs as opposed to paintings. In other words, these pre-trained models tend to be biased with regards to the classification of the paintings.\n",
    "\n",
    "In the next milestone, you will learn how to build your own model and address this type of issue.\n",
    "</span>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the best model\n",
    "\n",
    "Additional performance will be saved along with `Training Loss`, `Validations Loss`, `Accuracy`, `Class Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'training_losses': train_losses,\n",
    "              'valid_losses': valid_losses,\n",
    "              'accuracies': accuracies,\n",
    "              'classes_accuracies':classes_accuracies,\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, pathToModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional - Data visualization\n",
    "> **Note:** the below code is not part of the model training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(visualization_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "images.numpy()\n",
    "\n",
    "# move model inputs to cuda, if GPU available\n",
    "if train_on_gpu:    \n",
    "    images = images.to(device)\n",
    "    \n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "\n",
    "#move images to CPU for plotting\n",
    "images = images.cpu()\n",
    "\n",
    "# convert output probabilities to predicted class\n",
    "output_ps, preds_tensor = torch.max(output, 1)\n",
    "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(22, 22))\n",
    "for idx in np.arange(8):\n",
    "    ax = fig.add_subplot(5, 10/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(\"{} ({})\".format(artCategories[preds[idx]], artCategories[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}