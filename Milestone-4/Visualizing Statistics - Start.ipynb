{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 4 - Visualizing statistics\n",
    "\n",
    "---  \n",
    "\n",
    "# Objective of this notebook\n",
    "* Compare the performance of the 4 image classification models i.e. 3 different transfer learning algorithms and CNN model from scratch which we built in 2nd and 3rd milestones. \n",
    "* Plot and visualize different charts to compare the results which help to determine the best model.\n",
    "\n",
    "> **Note:** The content of this notebook follows the description provided in [Milestone-4](./Milestone-4.md)\n",
    "\n",
    "---\n",
    "## 1. Prerequisites\n",
    "\n",
    "### Importing packages & modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common modules/packages\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "# PyTorch modules/packages\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training mode based on CUDA capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'  # sets the default value\n",
    "train_on_gpu = torch.cuda.is_available()  # returns True if CUDA enabled GPU is available\n",
    "\n",
    "if train_on_gpu == True :\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    print('\\n')\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the list of files with a directory\n",
    "def getFilesInDirectory(pathToDir, extension = \"*.*\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return list(pathToDir.glob(extension))\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFoldersInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFolderNamesInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld.name for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the root folder for image sets\n",
    "pathToDataset = pathlib.Path.cwd().joinpath('..', 'dataset')\n",
    "pathToTrain = pathToDataset.joinpath('train')\n",
    "\n",
    "# Count and list art category\n",
    "artCategories = getFolderNamesInDirectory(pathToTrain, \".\")  #collects the list of folders\n",
    "print(\"Total no. of categories = \", len(artCategories))  #displays the number of classes (= Art categories)\n",
    "print(\"Categories: \", artCategories)  #displays the list of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the metrics\n",
    "\n",
    "* Retrieve for each model previously saved the performance metrics saved during the previous milestones. These metrics were saved as **checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining checkpoints\n",
    "checkpoints = []\n",
    "model_names = []\n",
    "\n",
    "# For each files within the model folder\n",
    "files = getFilesInDirectory(pathlib.Path.cwd().joinpath('..', 'models'), '*.pt')    #lists all the 'jpg' images in the folder\n",
    "for file in files:\n",
    "    # Extracting the model name\n",
    "    model_names.append(file.name[8:len(file.name)-3])\n",
    "    # Add to checkpoint library\n",
    "    checkpoints.append( torch.load(file) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "># TO DO:\n",
    ">* For each checkpoints, display the metrics. The metrics should be the same for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "#-------------------------------------------------------------------------#\n",
    "#------  Make sure that the statistics are the same for each model  ------#\n",
    "#-------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect  the performance metrics\n",
    "\n",
    "The following metrics were previously saved within a checkpoint:\n",
    "* Training loss over each epoch\n",
    "* Validation loss over each epoch\n",
    "* Accuracy over each epoch\n",
    "* Class accuracy (testing)\n",
    "* Number of epochs\n",
    "\n",
    "These metrics will be represented through several plots. The coordinates in the X axis will be represented by the number of Epoch.\n",
    "\n",
    "---\n",
    "># TO DO:\n",
    ">* For all checkpoints, create a list for each metrics\n",
    ">* Determine the maximum of all Epoch numbers (across all checkpoints) and taking the lowest\n",
    ">* Create an integer range between 1 and the maximum adn convert it into a list. This list represents the X axis coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to consider\n",
    "max_numEpoch = 40\n",
    "\n",
    "# Collecting statistics\n",
    "accuracies = []\n",
    "classes_accuracies = []\n",
    "num_epochs = []\n",
    "training_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Going through the collection of checkpoints\n",
    "for checkpoint in checkpoints:\n",
    "    training_losses. ...\n",
    "    valid_losses. ...\n",
    "    accuracies. ...\n",
    "    classes_accuracies. ..\n",
    "    num_epochs. ...\n",
    "\n",
    "epochs = list(range(1, max_numEpoch + 1, 1))\n",
    "#----------------------------------------------------------------------------------------------------------------------#\n",
    "#------  Note that `state_dict` is the trained model and is of no use while plotting and visualizing statistics  ------#\n",
    "#----------------------------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting - Learning Curves\n",
    "\n",
    "A learning curve is a plot of model learning performance over experience or time (= epoch). Each model has been evaluated against a training dataset and a validation dataset.\n",
    "\n",
    "Learning curves can facilitate the diagnosis of problems with learning, such as an underfit or overfit model, as well as whether the training and validation datasets are suitably representative.\n",
    "\n",
    "### Training & Validation Loss per model\n",
    "The train Learning Curve shows of how well the model is learning. The validation Learning Curve shows how well the model is generalizing.\n",
    "\n",
    "---\n",
    "># TO DO:\n",
    ">* For each model, create a graph with both train and validation curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the collection of checkpoints\n",
    "for i in range(len(model_names)):\n",
    "    x = ...\n",
    "    y = ...\n",
    "    z = ...\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss v/s Epoch' + '     (' + model_names[i] + ')')\n",
    "    plt.plot(x,y,label = 'Training Loss')\n",
    "    plt.plot(x,z,label = 'Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Training Loss for all models\n",
    "In order to evaluate which model learns best:\n",
    "---\n",
    "># TO DO:\n",
    ">* Create one graph for all the models for the train learning curve only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting coordinates\n",
    "x = epochs\n",
    "ys = []\n",
    "for i in range(len(model_names)):\n",
    "    ys...\n",
    "\n",
    "# Plotting\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss  v/s Epoch')\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    plt...\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Validation Loss for all models\n",
    "In order to evaluate which model generalizes best:\n",
    "\n",
    "---\n",
    "># TO DO:\n",
    ">* Create one graph for all the models for the validation learning curve only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting coordinates\n",
    "x = epochs\n",
    "ys = []\n",
    "for i in range(len(model_names)):\n",
    "    ys.append(valid_losses[i])\n",
    "\n",
    "# Plotting\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss  v/s Epoch')\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    plt.plot(x,ys[i],label = model_names[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Accuracy Loss for all models\n",
    "In order to evaluate which model performs best:\n",
    "* Create one graph for all the models for the performance learning curve only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting coordinates\n",
    "x = epochs\n",
    "ys = []\n",
    "for i in range(len(model_names)):\n",
    "    ys.append(accuracies[i])\n",
    "\n",
    "# Plotting\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy  v/s Epoch')\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    plt.plot(x,ys[i],label = model_names[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Accuracy Loss per art category\n",
    "In order to evaluate which model performs best per category:\n",
    "* For each model, create one graph for all the categories for the performance learning curve only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_names)):\n",
    "    plt.barh(artCategories,classes_accuracies[i],align = 'center')\n",
    "    plt.xlabel('Accuracy (%)')\n",
    "    plt.ylabel('Art Class')\n",
    "    plt.title('Class Accuracy' + '         (' + model_names[i] +')')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional - Data visualization\n",
    ">* Create one graph for all the categories (x-Axis) showing the performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    data.append(classes_accuracies[i])\n",
    "    \n",
    "X = np.arange(1,len(artCategories)+1,1)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,2,1])\n",
    "\n",
    "for i in range(len(data)):\n",
    "    ax.bar(X, data[i], width = 0.1, label = model_names[i])\n",
    "    X = X - 0.1\n",
    "\n",
    "#ax.bar(X + 0.30, data[3], color = 'y', width = 0.1, label = model4_name)\n",
    "\n",
    "plt.xlabel('Art ID') \n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print (\"{:<22}| {:<3}\".format('Art Name','Art ID'))\n",
    "print (\"----------------------+---------- \")\n",
    "for x , y in zip(artCategories , X):\n",
    "    print (\"{:<22}| {:.3f}\".format(x,y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
