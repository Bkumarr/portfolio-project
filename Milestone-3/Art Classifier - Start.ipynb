{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 3 - Implementing Customized Art Classifier\n",
    "\n",
    "---  \n",
    "\n",
    "# Objective of this notebook\n",
    "* Build a CNN model from scratch\n",
    "* Train the model on art paintings dataset\n",
    "* Compare and visualize its performance on different art classes\n",
    "\n",
    "> **Note:** The content of this notebook follows the description provided in [Milestone-3](./Milestone-3.md)\n",
    "\n",
    "---\n",
    "## 1. Prerequisites\n",
    "\n",
    "### Importing packages & modules\n",
    "You might prefer to load the required modules/packages when required. Feel free to do so if it is your preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common modules/packages\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import sys, shutil, time\n",
    "import warnings\n",
    "\n",
    "# PyTorch modules/packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training mode based on CUDA capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'  #sets the default value\n",
    "train_on_gpu = torch.cuda.is_available()  #returns True if CUDA enabled GPU is available\n",
    "\n",
    "if train_on_gpu == True :\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    print('\\n')\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some utilities functions  \n",
    "These functions will be used to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the list of files with a directory\n",
    "def getFilesInDirectory(pathToDir, extension = \"*.*\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return list(pathToDir.glob(extension))\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFoldersInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFolderNamesInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld.name for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "The folder structure created through Milestone 1 should looks like this:\n",
    "```\n",
    "dataset/train/artCategory_1/file_01.jpg\n",
    "dataset/train/artCategory_1/file_03.jpg\n",
    "dataset/train/artCategory_1/file_06.jpg\n",
    "...\n",
    "dataset/test/artCategory_1/file_02.jpg\n",
    "...\n",
    "dataset/valid/artCategory_1/file_04.jpg\n",
    "```\n",
    "\n",
    "The root folder for training is `dataset/train` and the classes are the names of art types.\n",
    "Likewise, `dataset/valid` and `dataset/test` for validation and testing respectively.\n",
    "\n",
    "> **Note:**\n",
    ">   - If you have downloaded images through Milestone 1, the expected folder structure above should be already present\n",
    ">   - If you have used the set of images (image-sets.zip) provided, please note these images have the following format: 432*288\n",
    "\n",
    "### Sets the folders\n",
    "* Set the location for `train`, `test` and `valid` folders\n",
    "* Count the number of Art category and list them using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE DATA DIRECTORIES & LOCATION OF IMAGE-SETS ARCHIVE\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# sets the root folder for image sets\n",
    "pathToDataset = pathlib.Path.cwd().joinpath('..','dataset')\n",
    "pathToTrain = pathToDataset.joinpath('train')\n",
    "pathToTest = pathToDataset.joinpath('test')\n",
    "pathToValid = pathToDataset.joinpath('valid')\n",
    "\n",
    "# count and list art category\n",
    "artCategories = getFolderNamesInDirectory(pathToTrain, \".\")  # collects the list of folders\n",
    "\n",
    "print(\"Total no. of categories = \", len(artCategories))\n",
    "print(\"Categories: \", artCategories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Let's assume that our model expects a `224`-dim square image as input. Resizing will therefore be required for each art image to fit this model. These transformations applies on `train`, `test` and `valid`. Use PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class, which makes it very easy to load data from a directory. \n",
    "\n",
    "***A. Data-Augmentation***  \n",
    "Using data augmentation on training images.\n",
    "1. Randomly rotating the training by 30 degress.\n",
    "2. Randomly cropping and resizing it to 224-dim square image.  \n",
    "    !!! Please use only one function that crops the given image to random size and aspect ratio.\n",
    "3. Randomly flipping it horizontally.\n",
    "\n",
    "> **Note:** \n",
    "Data augmentation isn't applied on validation and testing set. These images are resized to 256 pixels and then cropped from center to make it 224-dim square images.  \n",
    "\n",
    "***B. Normalization:***  \n",
    "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. Same normalization was used by these pre-trained models for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data-augmentation transforms including normalisations\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])])\n",
    "                 \n",
    "test_transforms  = transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])])\n",
    "                 \n",
    "valid_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# load and apply above transforms on dataset using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(pathToTrain, transform=train_transforms)\n",
    "test_dataset  = datasets.ImageFolder(pathToTest , transform=test_transforms)\n",
    "valid_dataset = datasets.ImageFolder(pathToValid, transform=valid_transforms)\n",
    "\n",
    "# Print out data stats\n",
    "print('Training  images: ', len(train_dataset))\n",
    "print('Testing   images: ', len(test_dataset))\n",
    "print('Validation images:', len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "A [data loader](https://pytorch.org/docs/stable/data.html) is an iterable over a dataset. The parameters are:\n",
    "* `batch`:  number of samples per batch to be loaded\n",
    "* `shuffle`: set to True, data are reshuffled at every epoch.\n",
    "* `num_workers`: nbr of subprocesses to use for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloader parameters\n",
    "batch_size = 16  # You might want to increase the size to 32. This might raise an exception \n",
    "num_workers = 0\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle = True)\n",
    "test_dataloader  = torch.utils.data.DataLoader(test_dataset , batch_size=batch_size, num_workers=num_workers, shuffle = False)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle = False)\n",
    "\n",
    "# Print the batches stats\n",
    "print('Number of  training  batches:', len(train_dataloader))\n",
    "print('Number of  testing   batches:', len(test_dataloader))\n",
    "print('Number of validation batches:', len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional - Data visualization\n",
    "> **Note:** the below code is not part of the model training/testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore normalization and turn shuffle ON to visualize different classes together\n",
    "visual_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "# Load and apply above transforms on dataset using ImageFolder\n",
    "# Use test directory images can be used for visualization\n",
    "visual_dataset = datasets.ImageFolder(pathToTest ,transform=visual_transforms)\n",
    "\n",
    "# Prepare data loaders\n",
    "visualization_dataloader = torch.utils.data.DataLoader(visual_dataset, batch_size = batch_size, num_workers = num_workers, shuffle=True)\n",
    "\n",
    "# Obtain one batch of testing images\n",
    "dataiter = iter(visualization_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Convert images to numpy for display\n",
    "images = images.numpy()\n",
    "\n",
    "# Plot the images in the batch along with the corresponding labels\n",
    "plotCols = 4\n",
    "plotRows = math.ceil(batch_size/plotCols) # SqRoot could be used as well: math.ceil(math.sqrt(batch_size))\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(plotRows, plotCols, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(artCategories[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelling\n",
    "\n",
    "### 3.1. Definition\n",
    "\n",
    "Define a model based on a class inherited from nn.Module which will take in a 224x224 dimensional image.\n",
    "\n",
    "1. Initialise a CNN model  \n",
    "This class function (`__init()__`) defines the architecture or flow of the model.  \n",
    "\n",
    "\t* 5 convolutional layers configured as such:  \n",
    "\t\t* input/output filters: 3, 8, 16, 32, 64, respectively\n",
    "\t\t* filter size: 224*224, 112*112, 56*56, 28*28, 14*14\n",
    "\n",
    "\t\tEach layer uses a kernel size of 3x3, stride and padding value of 1.\n",
    "\n",
    "\t* Max pooling layer with a kernel size of 2*2 and stride as 2. This layer takes the output of convolutional layer and decreases the dimensionality by half.\n",
    "\t* 3 fully connected layers with an output of 1024 and 256, `x` nodes, respectively. `x` represents the number of art classes processed.\n",
    "\t\n",
    "\tThe example below shows how to create a single layer.\n",
    "For ex: `self.conv1 = nn.Conv2d(3,  8, 3, stride = 1, padding = 1)` where the arguments are in_depth, out_depth, kernel_size, stride, padding. Here `self.conv1` is user define variable which denotes the given layer. In this manner, we can create multiple layers and assign each one to its respective variable name.\n",
    "\n",
    "2. Model Architecture/Flow  \n",
    "This class function (`forward()`) defines the architecture or flow of the model.\n",
    "\t* Using the layer variables created in the above init() function, provide a sequential pathway for the tensors to pass through the layers define earlier.\n",
    "\t* Provide the activation function applied on the layer.  For ex: `x = F.relu(self.conv1(x))`.\n",
    "\n",
    "\tThe flow is as follow:\n",
    "\t* Set the `relu` activation function to each convolutional layer\n",
    "\t* Set a max pooling layer after each convolutional layer\n",
    "\t* Set the `relu` activation function and dropout to each fully connected layers.\n",
    "\n",
    "---\n",
    "># TO DO (1):\n",
    ">1. Define a model based on the description above (see Definition)\n",
    ">2. Instantiate the model\n",
    ">3. Move the model to GPU (if available)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return x\n",
    "\n",
    "# instantiate the CNN\n",
    "...\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if train_on_gpu == True:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set the model name and its location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ArtClassifier'\n",
    "model_filename = 'trained_' + model_name + '.pt'\n",
    "pathToModel = pathlib.Path.cwd().joinpath('..', 'models', model_filename)\n",
    "print('File name for saved model: ', pathToModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "    Let's define a criterion and an optimizer that will work \"together\". The criterion will be used to stop the algorithm when it approaches the optimum.  \n",
    "\n",
    "    We will use [ADAM](https://arxiv.org/abs/1412.6980) algorithm as optimizer and use the Cross-Entropy Loss function as criterion. Note that the optimizer accepts as input _only_ the trainable parameters.\n",
    "\n",
    "---\n",
    "># TO DO (2):\n",
    ">## Define a criterion (CrossEntropyLoss) and an optimizer based on ADAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: select loss function\n",
    "\n",
    "\n",
    "### TODO: select optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Model Training\n",
    "\n",
    "1. Training Preparation  \n",
    "\n",
    "    The following variables will be defined:\n",
    "    * The number of epochs required for the training and validation of the model\n",
    "    * The minimal value for validation loss\n",
    "    * Some performance variables such as:\n",
    "        * the losses for training and validation\n",
    "        * the model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some images in dataset were truncated (maybe corrupted)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Set number of epochs to train the model\n",
    "n_epochs = 40\n",
    "\n",
    "# Initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf   # set initial \"min\" to infinity\n",
    "\n",
    "# Initialise performances\n",
    "train_losses, valid_losses, accuracies=[],[],[]\n",
    "training_loss = 0.0\n",
    "validation_loss = 0.0\n",
    "accuracy = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train & Validation the model\n",
    "\n",
    "    **Training**  \n",
    "    The training will be performed by going through every epoch.  \n",
    "\n",
    "    * Collect a set of labelled images and for each image:\n",
    "        * execute the `forward` method\n",
    "        * calculate the loss using the criterion and the gradient of the loss\n",
    "        * apply a single step optimization\n",
    "        * refresh the training loss\n",
    "\n",
    "    **Validate**  \n",
    "    The validation will be performed by going through every epoch - same as the training.\n",
    "\n",
    "    * Place the model into validation model\n",
    "        * execute the `forward` method\n",
    "        * calculate the validation loss and the accuracy\n",
    "        * compare your predictions against the true labels\n",
    "        * incrementing values of 'accuracy' with equals\n",
    "    * Save the model if validation loss has decreased\n",
    "\n",
    "    Lastly, for each epoch, print the performance variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()  #Start-time for training\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    c = time.time()  #Start-time for epoch\n",
    "    \n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "        \n",
    "    # model by default is set to train\n",
    "    for batch_i, (images, labels) in enumerate(train_dataloader): #Getting one batch of training images and their corresponding true labels\n",
    "        \n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "        # clear the previous/buffer gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model_scratch.forward(images)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(outputs, labels)    #(y_hat, y)  or (our-prediction, true-label)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update training loss \n",
    "        training_loss += loss.item()\n",
    "        \n",
    "    ##################\n",
    "    # VALIDATE MODEL #\n",
    "    ##################      \n",
    "    \n",
    "    #validation loss and accuracy\n",
    "    validation_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "\n",
    "    model_scratch.eval() #model is put to evalution mode i.e. dropout is switched off\n",
    "\n",
    "    with torch.no_grad():  #Turning off calculation of gradients (not required for validaiton)  {saves time}\n",
    "        for images, labels in valid_dataloader:   #Getting one batch of validation images\n",
    "            \n",
    "            if train_on_gpu:   #moving data to GPU if available\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            outputs = model_scratch.forward(images)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            validation_loss += batch_loss.item()\n",
    "            \n",
    "            # Calculating accuracy\n",
    "            ps = torch.exp(outputs)   #Turning raw output values into probabilities using exponential function\n",
    "\n",
    "            #getting top one probablilty and its corresponding class for batch of images\n",
    "            top_p, top_class = ps.topk(1, dim=1) \n",
    "\n",
    "            #Comparing our predictions to true labels\n",
    "            equals = top_class == labels.view(*top_class.shape)   #equals is a list of values\n",
    "\n",
    "            #incrementing values of 'accuracy' with equals\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()   #taking average of equals will give number of true-predictions\n",
    "                                                #equals if of ByteTensor (boolean), changing it to FloatTensor for taking mean...\n",
    "    \n",
    "    train_losses.append(training_loss/len(train_dataloader))    \n",
    "    valid_losses.append(validation_loss/len(valid_dataloader))\n",
    "    accuracies.append(((accuracy/len(valid_dataloader))*100.0))\n",
    "    d = time.time() #end-time for epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch} \"\n",
    "          f\"Time: {int((d-c)/60)} min {int(d-c)%60} sec \"\n",
    "          f\"Train loss: {training_loss/len(train_dataloader):.2f}.. \"\n",
    "          f\"Validation loss: {validation_loss/len(valid_dataloader):.2f}.. \"\n",
    "          f\"Validation accuracy: {((accuracy/len(valid_dataloader))*100.0):.2f}% \"\n",
    "          )\n",
    "\n",
    "    training_loss = 0.0\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if ( validation_loss/len(valid_dataloader) <= valid_loss_min):\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min , validation_loss/len(valid_dataloader)))\n",
    "        torch.save(model_scratch.state_dict(), pathToModel) #Saving model \n",
    "        valid_loss_min = validation_loss/len(valid_dataloader)   #Minimum validation loss updated\n",
    "\n",
    "    #After validation, model is put to training mode i.e. dropout is again switched on\n",
    "    model_scratch.train()\n",
    "\n",
    "    ################\n",
    "    # END OF EPOCH #\n",
    "    ################\n",
    "\n",
    "b = time.time()  #end-time for training\n",
    "print('\\n\\n\\tTotal training time: ' , int((b-a)/(60*60)), \"hour(s) \" , int(((b-a)%(60*60))/60),\"minute(s) \", int(((b-a)%(60*60))%60) , \"second(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scratch.load_state_dict(torch.load(pathToModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Testing\n",
    "\n",
    "1. Testing Preparation  \n",
    "\n",
    "    The following variables will be defined:\n",
    "    * The number of epochs required for the training and validation of the model\n",
    "    * The minimal value for validation loss\n",
    "    * Some performance variables such as:\n",
    "        * the losses for training and validation\n",
    "        * the model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "counter = 0\n",
    "\n",
    "class_correct = list(0. for i in range(len(artCategories)))\n",
    "class_total = list(0. for i in range(len(artCategories)))\n",
    "classes_accuracies=[]\n",
    "\n",
    "# evaluation mode (switching off dropout)\n",
    "model_scratch.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "2. Testing the model  \n",
    "\n",
    "    * Collect a set of labelled images and for each image:\n",
    "        * send each image to the model and collect the output\n",
    "        * based on the output and the label, evaluate the loss\n",
    "        * convert the output probabilities to a predicted class\n",
    "        * for each art category, calculate the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over test data - get one batch of data from testloader\n",
    "for images, labels in test_dataloader:\n",
    "    \n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "    # compute predicted outputs by passing inputs to the model\n",
    "    output = model_scratch(images)\n",
    "    \n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # update test loss \n",
    "    test_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Convert output probabilities to predicted class\n",
    "    ps, pred = torch.max(output, 1)    \n",
    "    \n",
    "    # Compare model's predictions to true labels\n",
    "    for i in range(len(images)):\n",
    "        class_total[labels[i]] += 1\n",
    "        if pred[i] == labels[i]:\n",
    "            class_correct[pred[i]] += 1\n",
    "    counter += 1\n",
    "\n",
    "# calculate average test loss\n",
    "test_loss = test_loss/len(test_dataloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(artCategories)):\n",
    "    classes_accuracies.append(100 * class_correct[i] / class_total[i])\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            artCategories[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    \n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (artCategories[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % \n",
    "                          (100. * np.sum(class_correct) / np.sum(class_total),np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the best model\n",
    "\n",
    "Additional performance will be saved along with `Training Loss`, `Validations Loss`, `Accuracy`, `Class Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'training_losses': train_losses,\n",
    "              'valid_losses': valid_losses,\n",
    "              'accuracies': accuracies,\n",
    "              'classes_accuracies':classes_accuracies,\n",
    "              'state_dict': model_scratch.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional - Data visualization\n",
    "> **Note:** the below code is not part of the model training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(visualization_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "images.numpy()\n",
    "\n",
    "# move model inputs to cuda, if GPU available\n",
    "if train_on_gpu:    \n",
    "    images = images.to(device)\n",
    "    \n",
    "# get sample outputs\n",
    "output = model_scratch(images)\n",
    "\n",
    "#move images to CPU for plotting\n",
    "images = images.cpu()\n",
    "\n",
    "# convert output probabilities to predicted class\n",
    "output_ps, preds_tensor = torch.max(output, 1)\n",
    "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
    "\n",
    "# Plot the images in the batch along with the corresponding labels\n",
    "plotCols = 4\n",
    "plotRows = math.ceil(batch_size/plotCols) # SqRoot could be used as well: math.ceil(math.sqrt(batch_size))\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(plotRows, plotCols, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(\"{} ({})\".format(artCategories[preds[idx]], artCategories[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}