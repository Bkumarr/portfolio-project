{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMdbwOEbeQ0s"
   },
   "source": [
    "# Milestone 1 - Data Collection and Preparation  \n",
    "\n",
    "---  \n",
    "\n",
    "# Objective of this notebook\n",
    "* Prepare the image sets before modelling phase\n",
    "\n",
    "> **Note:** The content of this notebook follows the description provided in [Milestone-1](./Milestone-1.md)\n",
    "\n",
    "---\n",
    "## 1. Importing packages & modules\n",
    "You might prefer to load the required modules/packages when required. Feel free to do so if it is your preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common modules/packages\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib, os, shutil\n",
    "import random\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "\n",
    "from csv import reader\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "Considering that modelling requires 3 image sets ('training', 'testing', 'valid'), a decent number of images must be collected beforehand. We propose three methods to download images. Nevertheless, we recommend to use the first method as the other milestones have been built on it.\n",
    "\n",
    "Nevertheless, it is worth understand how images can be retrieved, we therefore encourage to look at method 2.2. and 2.3.\n",
    "\n",
    "### 2.1. Extract from archive file (recommended)\n",
    "There is a set of images (downloads.zip) provided located in Dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToDataset = pathlib.Path.cwd().joinpath('..', 'dataset')\n",
    "os.chdir(pathToDataset)\n",
    "\n",
    "images_file = os.path.join(pathToDataset, 'downloads.zip')\n",
    "\n",
    "# Extracting all the images to `train` folder\n",
    "with ZipFile(images_file, 'r') as zipObj:\n",
    "   zipObj.extractall(pathToDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Direct download\n",
    "The method below helps you to be more specific whether you want to focus on an art category or you already have a list of images. This method assumes that you have a list of files containing links to images. We provide with two files as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Download images from a list of urls\n",
    "def download_listed_images(filepath):\n",
    "\n",
    "    # Check 'downloads' folder exists\n",
    "    pathToDownload = pathlib.Path.cwd().joinpath('downloads')\n",
    "    if not pathToDownload.exists():\n",
    "        pathToDownload.mkdir()\n",
    "\n",
    "    # Check Art Category folder exists\n",
    "    pathToDownload = pathToDownload.joinpath(filepath[:-4])\n",
    "    if not pathToDownload.exists():\n",
    "        pathToDownload.mkdir()\n",
    "    \n",
    "    # grab the list of URLs from the input file, then initialize the total number of images downloaded so far\n",
    "    urls = open(filepath).read().strip().split(\"\\n\")\n",
    "    urlCounter = 0\n",
    "\n",
    "    # loop the URLs\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # try to download the image\n",
    "            req = requests.get(url, timeout=60)\n",
    "\n",
    "            # save the image to disk\n",
    "            pathToDownloadedImage = pathToDownload.joinpath(\"{}.jpg\".format(str(urlCounter).zfill(8)))\n",
    "            downloaded_image = open(pathToDownloadedImage, \"wb\")\n",
    "            downloaded_image.write(req.content)\n",
    "            downloaded_image.close()\n",
    "\n",
    "            # update the counter\n",
    "            print(\"[INFO] downloaded: {}\".format(pathToDownloadedImage))\n",
    "            urlCounter += 1\n",
    "            \n",
    "        # handle if any exceptions are thrown during the download process\n",
    "        except:\n",
    "            print(\"[INFO] error downloading {}...skipping\".format(pathToDownloadedImage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the 'download_listed_images' function with a list of files containing the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "pathToDataset = pathlib.Path.cwd().joinpath('..', 'dataset')\n",
    "os.chdir(pathToDataset)\n",
    "\n",
    "###\n",
    "# List of files\n",
    "image_files = ['cubism.txt', 'surrealism.txt']\n",
    "for image_file in image_files:\n",
    "    download_listed_images(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "Now that the images are downloaded, let's prepare the datasets. \n",
    "\n",
    "For example, the training images are all stored in a directory path that looks like this:\n",
    "```\n",
    "dataset/train/artCategory_1/abc123.jpg\n",
    "dataset/train/artCategory_1/abc456.jpg\n",
    "dataset/train/artCategory_1/abc789.jpg\n",
    "...\n",
    "dataset/train/artCategory_2/abc123.jpg\n",
    "dataset/train/artCategory_2/abc456.jpg\n",
    "dataset/train/artCategory_2/abc789.jpg\n",
    "```\n",
    "\n",
    "Where, in this case, the root folder for training is `dataset/train` and the classes are the names of art types. Likewise, `dataset/valid` and `dataset/test` for validation and testing respectively.\n",
    "\n",
    "## 3.1. Preparation functions\n",
    "\n",
    "Before spreading the images, let's create two utilities functions:\n",
    "* One function should return a list of files present in a specific directory\n",
    "* One function should return a sorted list of folder names present in a specific directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the list of files with a directory\n",
    "def getFilesInDirectory(pathToDir, extension = \"*.*\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return list(pathToDir.glob(extension))\n",
    "\n",
    "# Retrieves the list of folders with a directory\n",
    "def getFolderNamesInDirectory(pathToDir, prefix = \"\"):\n",
    "    if not isinstance(pathToDir, pathlib.PurePath):\n",
    "        pathToDir = pathlib.Path(pathToDir)\n",
    "\n",
    "    return sorted([fld.name for fld in pathToDir.iterdir() if fld.is_dir() and not fld.name.lower().startswith(prefix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Prepare the images\n",
    "* Set the location for `train`, `test` and `valid` folders and create the missing folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Sets the root folder for image sets\n",
    "pathToDataset  = pathlib.Path.cwd()\n",
    "pathToDownload = pathToDataset.joinpath('downloads')\n",
    "\n",
    "pathToTrain = pathToDataset.joinpath('train')\n",
    "if not pathToTrain.exists():\n",
    "    pathToTrain.mkdir()\n",
    "\n",
    "pathToTest = pathToDataset.joinpath('test')\n",
    "if not pathToTest.exists():\n",
    "    pathToTest.mkdir()\n",
    "\n",
    "pathToValid = pathToDataset.joinpath('valid')\n",
    "if not pathToValid.exists():\n",
    "    pathToValid.mkdir()\n",
    "\n",
    "###\n",
    "# Sets the folder for models (where all the models will be saved)\n",
    "pathToModels = pathToDataset.joinpath('..', 'models')\n",
    "if not pathToModels.exists():\n",
    "    pathToModels.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count the number of Art category and list them using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the folders required under 'dataset' folder (using a list to reduce the lines of code)\n",
    "artCategories = getFolderNamesInDirectory(pathToDownload, \".\")  #collects the list of folders\n",
    "print(\"Total no. of categories = \", len(artCategories))  #displays the number of classes (= Art categories)\n",
    "print(\"Categories: \", artCategories)  #displays the list of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each art category in the downloads folder, spread the images to `test` folder (20% of them) and `valid` folder (20% of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each art category\n",
    "for artCategory in artCategories:\n",
    "\n",
    "    # Sets the source folder\n",
    "    path_source = pathToDownload.joinpath(artCategory)\n",
    "    \n",
    "    # Sets the datasets\n",
    "    files = getFilesInDirectory(path_source, '*.jpg')    # lists all the 'jpg' images in the folder\n",
    "    random.shuffle( files )\n",
    "    split_idx = int(round(len(list(files)) / 5, 0))      # Determines the splitting index: 5 = 20%\n",
    "    split_images = np.split(files, [3*split_idx, 4*split_idx, 5*split_idx]) # Split the files across the 3 datasets\n",
    "\n",
    "    # Sets the target folders\n",
    "    path_target_train = pathToTrain.joinpath(artCategory)\n",
    "    if not path_target_train.exists():\n",
    "        path_target_train.mkdir()\n",
    "    for img_file in split_images[0]:\n",
    "        shutil.move(img_file, path_target_train.joinpath(img_file.name))    \n",
    "            \n",
    "    path_target_test = pathToTest.joinpath(artCategory)\n",
    "    if not path_target_test.exists():\n",
    "        path_target_test.mkdir()\n",
    "    for img_file in split_images[1]:\n",
    "        shutil.move(img_file, path_target_test.joinpath(img_file.name))    \n",
    "\n",
    "    path_target_valid = pathToValid.joinpath(artCategory)\n",
    "    if not path_target_valid.exists():\n",
    "        path_target_valid.mkdir()\n",
    "    for img_file in split_images[2]:\n",
    "        shutil.move(img_file, path_target_valid.joinpath(img_file.name))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check the folder content\n",
    "\n",
    "You should have the following structure:\n",
    " * (image-segmentation) >  dataset >  downloads  \n",
    " * (image-segmentation) >  dataset >  test  \n",
    " * (image-segmentation) >  dataset >  train  \n",
    " * (image-segmentation) >  dataset >  valid  \n",
    "\n",
    "With each of the 'train', 'test' and 'valid' folders, you should retrieve one folder per art category containing the images. Some of the downloaded image files might be corrupted or simply not images. The code below removes these files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanImages(location):\n",
    "    artCategories = getFolderNamesInDirectory(location, \".\")\n",
    "\n",
    "    # For each art category\n",
    "    for artCategory in artCategories:\n",
    "\n",
    "        # Sets the source folder\n",
    "        path_source = pathToTrain.joinpath(artCategory)\n",
    "\n",
    "        # Sets the datasets\n",
    "        files = getFilesInDirectory(path_source, '*.jpg')    # lists all the 'jpg' images in the folder\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                img = Image.open(file)\n",
    "            except IOError:\n",
    "                print( file )\n",
    "                os.remove(file)\n",
    "\n",
    "pathToTrain = pathlib.Path.cwd().joinpath('..', 'dataset', 'train')\n",
    "cleanImages(pathToTrain)\n",
    "\n",
    "pathToValid = pathlib.Path.cwd().joinpath('..', 'dataset', 'valid')\n",
    "cleanImages(pathToValid)\n",
    "\n",
    "pathToTest = pathlib.Path.cwd().joinpath('..', 'dataset', 'test')\n",
    "cleanImages(pathToTest)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "intro-seg.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "name": "python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}